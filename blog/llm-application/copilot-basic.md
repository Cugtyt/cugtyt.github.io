# 一个应用的Copilot要怎么做 - 简单讨论

## Contact me

* Blog -> <https://cugtyt.github.io/blog/llm-application/index>
* Email -> <cugtyt@qq.com>
* GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

本文将介绍如果要给一个应用做Copilot要怎么做

Copilot是微软给Office、Windows等软件应用集成LLM的一系列产品，Copilot与原来的软件共生共存，
这也是比较直接接入LLM的一种方式。那么，我们可以思考如果要给自己的软件加类似的Copilot要做哪些。

## 提供什么样的功能

Copilot从名字可以知道是辅助主软件而生的，它提供的功能是让用户更方便的使用主软件，主软件如果是工具类
软件会更加契合Copilot的功能。

第一种Copilot的功能是来自LLM对文本的处理能力，例如总结文章，修改文字表述，生成邮件等。
这类功能更加灵活，容错性更强，比较难出现完全让用户完全不满意的结果，用户会对结果稍加修改
然后采用。这类功能一般是主软件之前不具备的独立性功能，主要是LLM本身附加的。

第二种Copilot最常见的功能可以是通过对话形式对主软件的功能进行操作，输入你想要完成的事情，
然后调用主软件的接口运行，例如改变PPT的字体样式，设置段落格式，这类工作可以在没有Copilot
的情况下一样是可以完成的，可能就是需要手动做一些操作步骤。对于这样的功能，Copilot
提供的价值是提供方便，提高效率，减少人工。但同时，这类功能的风险在于LLM不一定能准确的理解
用户的真正意图，可能是用户没有清晰输入，也可能LLM本身对语言的理解还不足够，这将导致用户
对执行的结果不满意。

## 功能要怎么做

上面说的两种功能复杂性是不一样的，这里分开讨论。

### 来自LLM的功能

第一种功能主要是来自LLM对文本的处理能力，生成内容，总结文章等等。这种功能属于LLM特有且独立的功能，
因此实现起来也更加简单直接。对于主软件，其实旁边附带一个对话界面，当用户输入问题或者要求后，
直接调用LLM就可以做到。这种功能和主软件的交互相对简单，联系可能更多是将回答输入到主软件中，
或者从主软件中提取一些信息作为LLM的信息补充。

目前大多数LLM都会支持对话形式的API调用，可能是作为一个独立的软件使用，Copilot的区别是要集成到
主软件中，作为主软件的附加产品一同使用。

### 对主软件功能进行调用操作

这一类功能需要联系LLM的逻辑推理能力和主软件提供的功能接口共同完成，因此比上一类功能的实现更加复杂。
这种功能的实现需要和主软件有密集的交互，往往不是主软件原生支持的。这里将分为两大块来展开说说：

#### 主软件的功能接口

一般来说，主软件是没有天生为Copilot的情况暴露功能接口的，或者主软件设计要为Copilot这类功能留出空间，
那首先要解决的一个问题就是功能接口的暴露。

功能接口暴露出来才能让LLM理解主软件的功能和使用逻辑，对于Office套件来说，可能是样式的接口，段落的接口，
字体的接口等等。将这些接口信息进行清晰描述和暴露接口调用，这样当用户通过LLM触发功能请求的时候，
才能知道解决方案可以通过哪些接口来完成，才能让LLM进行推理辅助。

有了接口是第一步，这里先假设LLM能正确推理，那推理完成后得到了一系列接口的调用方案，怎么执行方案就变成
下一个要考虑的问题，不执行相当于只让LLM思考而不采取动作，这只能告诉用户有了想法但没法做。

因此这块的主要工作在于功能接口暴露+功能调用执行，这完全是为了适配LLM需要完成的前提条件。

### LLM的推理交互

有了主软件的功能接口，LLM才能排上用场。当用户对LLM提出需求的时候，LLM就需要拿到主软件的功能接口，
这里的功能接口需要包含清晰的功能描述和适用场景，这样便于LLM将用户需求转译成功能调用序列。

LLM在转成功能调用序列后，需要通过上面提到的调用执行来变成实际的执行结果，这只是LLM推理的第一步。
在看到结果后，LLM需要继续判断是否需要调用更多的功能接口，用户的需求是否已经完成。如果需要将继续推理，
生成新的功能调用方案，继续执行，直到满足一定的条件，这个条件可能是达到执行次数上限，也可能是LLM
觉得用户的要求已经达成可以退出。

对于这个推理过程中的接口调用，接口有两种类别可以考虑实现，一种是中间接口，意思是执行这个接口后
还是会继续处于推理状态，LLM推理并未结束，另一种是终点接口，意思是如果一旦执行这个接口，推理
过程将结束，不再继续当前推理。这里提到的推理是指一轮对话中LLM背后的运行，再一轮对话后再开始新一轮
对话将重复上述功能。

可以看到，功能接口层可以看作是主软件面向LLM的适配层，将自己的功能接口和执行暴露出来，LLM面向适配层
进行推理。

在Copilot的工程实践中，不止聊天窗口对话可以借助LLM来辅助，还有很多细节功能点来做，比如在检测到某种条件
时可以触发一系列预定义的流程，让LLM作为流程中的一环等等，这里先不展开。

下次有空再写。。。


> 如果有用欢迎打赏

![](../buymeacoffee.jpg)